{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaned Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CleanedDataset(.csv): https://drive.google.com/open?id=1pyyNswPtU9J6vYGZGAJss_u7704S82Do\n",
    "ExperimentResults(.csv):https://docs.google.com/spreadsheets/d/1sYVVqwnPrhTWFtEl3k4xBjvTRO3fccQcOHQkYHB-_Vs/edit?usp=sharing\n",
    "ResearchWork(.pdf):https://drive.google.com/open?id=1ptPcXJNDfrtKmhRkHFp_MmMNAzkDs1SX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the new one\n",
    "data = pd.read_csv('clean_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Missing value: Drop the reviews with missing value directly.\n",
    "data.dropna(inplace = True)\n",
    "data.drop('Unnamed: 0', 1, inplace =True)\n",
    "data.reset_index(drop = True, inplace = True)\n",
    "#500717 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-side Reviews: Remove the review with \"No Negative\"/\"No Positive\". \n",
    "data['NegativeReview'].replace('No Negative', \"\", inplace = True)\n",
    "data['PositiveReview'].replace('No Positive', \"\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine: Positive and negative reviews would be treated as only a review, and in addition : lower the case. \n",
    "corpus = data.NegativeReview + data.PositiveReview\n",
    "data.insert(0, \"Review\", corpus.str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization: Conduct the work_tokenize first. (sent_tokenizing is more complicated in this case.)\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "#data = data.join(data.TEXT.apply(sent_tokenize).rename('SENTENCES'))\n",
    "#sent = data.NegativeReview.apply(sent_tokenize)\n",
    "word_tokenized = data.Review.apply(word_tokenize)\n",
    "data.insert(0, \"WordToken\", word_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WordToken</th>\n",
       "      <th>Review</th>\n",
       "      <th>NegativeReview</th>\n",
       "      <th>PositiveReview</th>\n",
       "      <th>TripStyle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[i, am, so, angry, that, i, made, this, post, ...</td>\n",
       "      <td>i am so angry that i made this post available...</td>\n",
       "      <td>I am so angry that i made this post available...</td>\n",
       "      <td>Only the park outside of the hotel was beauti...</td>\n",
       "      <td>Leisure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[no, real, complaints, the, hotel, was, great,...</td>\n",
       "      <td>no real complaints the hotel was great great ...</td>\n",
       "      <td></td>\n",
       "      <td>No real complaints the hotel was great great ...</td>\n",
       "      <td>Leisure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[rooms, are, nice, but, for, elderly, a, bit, ...</td>\n",
       "      <td>rooms are nice but for elderly a bit difficul...</td>\n",
       "      <td>Rooms are nice but for elderly a bit difficul...</td>\n",
       "      <td>Location was good and staff were ok It is cut...</td>\n",
       "      <td>Leisure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[my, room, was, dirty, and, i, was, afraid, to...</td>\n",
       "      <td>my room was dirty and i was afraid to walk ba...</td>\n",
       "      <td>My room was dirty and I was afraid to walk ba...</td>\n",
       "      <td>Great location in nice surroundings the bar a...</td>\n",
       "      <td>Leisure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[you, when, i, booked, with, your, company, on...</td>\n",
       "      <td>you when i booked with your company on line y...</td>\n",
       "      <td>You When I booked with your company on line y...</td>\n",
       "      <td>Amazing location and building Romantic setting</td>\n",
       "      <td>Leisure</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           WordToken  \\\n",
       "0  [i, am, so, angry, that, i, made, this, post, ...   \n",
       "1  [no, real, complaints, the, hotel, was, great,...   \n",
       "2  [rooms, are, nice, but, for, elderly, a, bit, ...   \n",
       "3  [my, room, was, dirty, and, i, was, afraid, to...   \n",
       "4  [you, when, i, booked, with, your, company, on...   \n",
       "\n",
       "                                              Review  \\\n",
       "0   i am so angry that i made this post available...   \n",
       "1   no real complaints the hotel was great great ...   \n",
       "2   rooms are nice but for elderly a bit difficul...   \n",
       "3   my room was dirty and i was afraid to walk ba...   \n",
       "4   you when i booked with your company on line y...   \n",
       "\n",
       "                                      NegativeReview  \\\n",
       "0   I am so angry that i made this post available...   \n",
       "1                                                      \n",
       "2   Rooms are nice but for elderly a bit difficul...   \n",
       "3   My room was dirty and I was afraid to walk ba...   \n",
       "4   You When I booked with your company on line y...   \n",
       "\n",
       "                                      PositiveReview TripStyle  \n",
       "0   Only the park outside of the hotel was beauti...   Leisure  \n",
       "1   No real complaints the hotel was great great ...   Leisure  \n",
       "2   Location was good and staff were ok It is cut...   Leisure  \n",
       "3   Great location in nice surroundings the bar a...   Leisure  \n",
       "4    Amazing location and building Romantic setting    Leisure  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Anomaly, Weird records: Drop the empty review(or we could remove the review with less than 5 words?)\n",
    "word_count = data.WordToken.apply(lambda x: len(x))\n",
    "filter_count = (word_count >= 1) #if 5, remained 462350\n",
    "data = data[filter_count]\n",
    "data.reset_index(drop = True, inplace = True)\n",
    "#Now the total records are 500487"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#StopWordRemoval: Remove the NLTK build-in stopwords in all the records.\n",
    "#from nltk.corpus import stopwords\n",
    "#stop_words = set(stopwords.words('english'))\n",
    "#wosw = data.WordToken.apply(lambda x:  [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization: Convert the terms with different representations into the original.\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "def lemmatize_text(text):\n",
    "    return [wnl.lemmatize(w) for w in text]\n",
    "\n",
    "data.insert(0, 'Lemmatized',data.WordToken.apply(lemmatize_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add the word count feature.\n",
    "data.insert(0, 'WordCount', data.WordToken.apply(lambda x: len(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split dataset into train/test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/frame.py:3697: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "#Split them into train/test set, randomly with the test size 0.33 (same distribution of each class)\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(data, stratify = data['TripStyle'], test_size=0.33, random_state = 1)\n",
    "\n",
    "col = ['NegativeReview', 'PositiveReview']\n",
    "train.drop(col, 1, inplace = True)\n",
    "test.drop(col, 1, inplace = True)\n",
    "\n",
    "train.reset_index(drop = True, inplace = True)\n",
    "test.reset_index(drop = True, inplace = True)\n",
    "\n",
    "#335K v.s. 165K(train/test split with stratified 0.83 of majority)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Undersampling\n",
    "#from imblearn.under_sampling import RandomUnderSampler\n",
    "#rus = RandomUnderSampler(random_state = 1)\n",
    "#x_train, lbl_train = rus.fit_resample(x_train, lbl_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DOING SOME OUTLIER REMOVAL IN TRAINING SET\n",
    "#word_count = train.WordToken.apply(lambda x: len(x))\n",
    "#filter_count = (word_count >= 3) #if 5, remained 462350\n",
    "#train = train[filter_count]\n",
    "#train.reset_index(drop = True, inplace = True)\n",
    "#Now the total records are 500487\n",
    "#325729 now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Labeling\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "lbl_train, lbl_test = le.fit_transform(train.TripStyle), le.transform(test.TripStyle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "A. Origianl Framework\n",
    "    1. TFIDF\n",
    "    2. W2V\n",
    "    \n",
    "B. Experiment\n",
    "    1. One-class fitting: minority\n",
    "    2. Combination: WordCount included\n",
    "    3. POS filtering\n",
    "    4. Preprocessing: NER like tokenized\n",
    "    5. Combination: TFIDF + W2V \n",
    "    \n",
    "C. Feature Selection\n",
    "    1. Supervised Approach: ANOVA F test: f_classif\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original Experiment: Testing Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TFIDF original\n",
    "\n",
    "def dum(doc):\n",
    "    return doc\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "                             ngram_range = (1,1),\n",
    "                             tokenizer = dum, \n",
    "                             preprocessor = dum,\n",
    "                             min_df = 0.01)\n",
    "\n",
    "x_ = vectorizer.fit_transform(train.WordToken)\n",
    "\n",
    "#Implement GC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "idf_dict = vectorizer.idf_[:, np.newaxis]\n",
    "ss = StandardScaler()\n",
    "weight = ss.fit_transform(idf_dict)\n",
    "\n",
    "dictionary = dict(zip(vectorizer.get_feature_names(), weight.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idfweight(termlist):\n",
    "    wl = []\n",
    "    for word in termlist:\n",
    "        if word in dictionary.keys():\n",
    "            wl.append(dictionary[word])\n",
    "        else:\n",
    "            wl.append(1)\n",
    "    return wl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#W2V origianl\n",
    "\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "model = Word2Vec(size = 300, window = 2, min_count = 1)\n",
    "#In order to count the single-word review, cfg with two word window and one count.\n",
    "\n",
    "#Building the dictionary.\n",
    "model.build_vocab(train.WordToken)\n",
    "\n",
    "#Train the w2v model with WordWokens\n",
    "\n",
    "model.train(train.WordToken, total_examples = len(train.WordToken), epochs = 3)\n",
    "model.init_sims(replace = True)\n",
    "\n",
    "#To lower the memory usage, save/load in KV model is necessary...\n",
    "fpath = get_tmpfile(\"w2v.kv\")\n",
    "model.wv.save(fpath)\n",
    "del model\n",
    "wv = KeyedVectors.load(fpath, mmap='r')\n",
    "\n",
    "#Document vector: By simply get the average of the wordvec in the single doc.\n",
    "def doc_vec(doc, mean = np.zeros(wv.vector_size)):  \n",
    "    doc = [word for word in doc] # target input list of words\n",
    "    #v = wv[doc] #orginal vector from word embedding\n",
    "    #w = np.array(idfweight(doc))[:, np.newaxis] #idf-weighting approach\n",
    "    try:\n",
    "        return np.mean(wv[doc], axis = 0)\n",
    "       # return np.mean(np.multiply(wv[doc] , np.array(idfweight(doc))[:, np.newaxis]), axis = 0)\n",
    "    except:\n",
    "        return mean  #13168/165161 is zero vector\n",
    "\n",
    "x_train = np.vstack(train.WordToken.apply(doc_vec))  #not sure abou whether a more efficient way\n",
    "mean = np.mean(x_train, axis = 0)\n",
    "\n",
    "#TRY TO DO SOMETHING ON UNKNOWN WORD\n",
    "x_test = np.vstack(test.WordToken.apply(lambda x : doc_vec(x, mean) ))  #not sure abou whether a more efficient way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (1st NOW) Experiment I - Fit on Business-Labeled data only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorize II-3: Bow with TFIDF transformation.\n",
    "\n",
    "def dum(doc):\n",
    "    return doc\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "                             ngram_range = (1,2),\n",
    "                             tokenizer = dum, \n",
    "                             preprocessor = dum,\n",
    "                             min_df = 0.001,\n",
    "                             max_df = 0.4)\n",
    "\n",
    "bt = train[train.TripStyle == 'Business']\n",
    "x = vectorizer.fit(bt.WordToken)\n",
    "x_ = vectorizer.transform(train.WordToken)\n",
    "x_train = x_.toarray()\n",
    "x_test = vectorizer.transform(test.WordToken).toarray()\n",
    "\n",
    "#Implement GC\n",
    "del x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment II - WordCount included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, KBinsDiscretizer\n",
    "\n",
    "#a = scaler.fit_transform(train.WordCount[:, np.newaxis])  # fit does nothing.\n",
    "#b = scaler.transform(test.WordCount[:, np.newaxis])\n",
    "a = np.array(train.WordCount).reshape(len(train.WordCount), 1) # fit does nothing.\n",
    "b = np.array(test.WordCount).reshape(len(test.WordCount), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [],
   "source": [
    "kbin = KBinsDiscretizer(n_bins = 17, encode = 'onehot-dense', strategy = 'quantile')\n",
    "a = kbin.fit_transform(a) \n",
    "b = kbin.transform(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cobiine the wordcount in to be treated as the feature.\n",
    "x_train = np.append(x_train, a, axis = 1)\n",
    "x_test = np.append(x_test, b , axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment III - Part of speech filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#POS filter: Remaining some important words only, filtered others.\n",
    "#Warning: It took lots of time\n",
    "\n",
    "def pos_tagging(sent):\n",
    "    tagfilter = {'NOUN'}\n",
    "    target = [item for (item, tag) in nltk.pos_tag(sent, tagset = 'universal') if tag in tagfilter]\n",
    "    return target\n",
    "\n",
    "data.insert(0, 'POS', data.Lemmatized.apply(pos_tagging))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Selection: Fitting the ANOVA stats on training data(only doing on testing data.)\n",
    "from sklearn.feature_selection import f_classif, SelectKBest, VarianceThreshold, chi2\n",
    "\n",
    "def FeatureSelect_f(feature, target, d):\n",
    "    fs = SelectKBest(f_classif, k = d).fit(feature, target)\n",
    "    return fs\n",
    "\n",
    "#Supervised approach\n",
    "fs = FeatureSelect_f(x_train, lbl_train, 500)\n",
    "#Transform both training/testing set into new selected-form.\n",
    "x_train = fs.transform(x_train)\n",
    "x_test = fs.transform(x_test)\n",
    "\n",
    "#Unsupervised approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment IV - Preprocess work, special terms tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NumTokenizedl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-667-9f31c73a9b2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\d+\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"<num>\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NER'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWordToken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNumTokenizedl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'NumTokenizedl' is not defined"
     ]
    }
   ],
   "source": [
    "def NumTokenized(sent):\n",
    "    regex = \"\\d\"\n",
    "    t = [re.sub(\"\\d+\", \"<num>\", line) for line in sent]\n",
    "    return t \n",
    "data.insert(0, 'NER', data.WordToken.apply(NumTokenized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment V - Just combine them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Execute each of the vectorization approach to two variables\n",
    "#Assign the new feature vectors(combination) to x_train, x_test\n",
    "x_train = np.append(x_train1, x_train, axis = 1)\n",
    "x_test = np.append(x_test1, x_test, axis = 1)\n",
    "#Dimentionality is 300+434 = 734"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "-Supervised\n",
    "1. Naive Bayes: Gaussian\n",
    "\n",
    "\n",
    "-Unsupervised\n",
    "1. LDA\n",
    "2. Sentense clustering\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf_gnb = GaussianNB()\n",
    "clf_gnb.fit(x_train, lbl_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "1. Original\n",
    "2. Imblanced Problem included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation function:\n",
    "from sklearn.metrics import precision_recall_curve, auc, confusion_matrix, accuracy_score, classification_report\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "import matplotlib.pyplot as plt\n",
    "#ref: https://acutecaretesting.org/en/articles/precision-recall-curves-what-are-they-and-how-are-they-used\n",
    "def evaluating(truth, pred, ax=object):\n",
    "  \n",
    "    print(accuracy_score(truth, pred))\n",
    "    print(classification_report_imbalanced(truth, pred))    \n",
    "    print(confusion_matrix(truth, pred))\n",
    "    precision, recall, threshold = precision_recall_curve(truth, pred)\n",
    "\n",
    "    ax.step(recall, precision, color='b', alpha=1, where='post')\n",
    "    ax.fill_between(recall, precision, step='post', alpha=0.5, color='b')\n",
    "    ax.set_xlabel('Recall')\n",
    "    ax.set_ylabel('Precision')\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_title('Precision-Recall curve')\n",
    "    return ax\n",
    "#ref: https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7449034578381095\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.31      0.43      0.81      0.36      0.59      0.34     27350\n",
      "          1       0.88      0.81      0.43      0.84      0.59      0.36    137811\n",
      "\n",
      "avg / total       0.78      0.74      0.49      0.76      0.59      0.36    165161\n",
      "\n",
      "[[ 11812  15538]\n",
      " [ 26594 111217]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x182c00e48>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, (ax1) = plt.subplots(1, 1, figsize=(10,10))\n",
    "\n",
    "pred_gnb = clf_gnb.predict(x_test)\n",
    "evaluating(lbl_test, pred_gnb, ax1)\n",
    "\n",
    "# Imbalanced Problem\n",
    "#137K v.s. 27K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Approach: LDA modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_sentences = [nltk.pos_tag(sentence) for sentence in test]\n",
    "ne_chunked_sents = [nltk.ne_chunk(tagged) for tagged in tagged_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(train.Lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = train.Lemmatized\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-318-955d1b1fd155>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                            \u001b[0mpasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                                            \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                                            per_word_topics=True)\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0muse_numpy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatcher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunks_as_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_numpy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minit_dir_prior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, corpus, chunksize, decay, offset, passes, update_every, eval_every, iterations, gamma_threshold, chunks_as_numpy)\u001b[0m\n\u001b[1;32m    975\u001b[0m                         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"reached the end of input; now waiting for all remaining jobs to finish\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m                         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrho\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpass_\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m                     \u001b[0;32mdel\u001b[0m \u001b[0mother\u001b[0m  \u001b[0;31m# frees up memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mdo_mstep\u001b[0;34m(self, rho, other, extra_pass)\u001b[0m\n\u001b[1;32m   1021\u001b[0m         \u001b[0;31m# update self with the new blend; also keep track of how much did\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m         \u001b[0;31m# the topics change through this update, to assess convergence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1023\u001b[0;31m         \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpElogbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1024\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrho\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0mdiff\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_Elogbeta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus = corpus,\n",
    "                                           id2word = id2word,\n",
    "                                           num_topics = 20, \n",
    "                                           random_state = 100,\n",
    "                                           update_every = 1,\n",
    "                                           chunksize = 100,\n",
    "                                           passes = 10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
